{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fPP77DqlWQLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Start"
      ],
      "metadata": {
        "id": "3dLbZ6tcWVUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XwIh0yWWimv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d12b121-a6f1-4240-830a-bf5715952ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load token from file\n",
        "import os\n",
        "with open('/content/drive/MyDrive/hf_token.txt', 'r') as file:\n",
        "    hf_token = file.read().strip()"
      ],
      "metadata": {
        "id": "PAUpUj0swDyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text extraction from the pdfs - run if required"
      ],
      "metadata": {
        "id": "zJVbVdms_X2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "ikeFjpq9_Q9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import os\n",
        "# Folder path containing multiple PDF files\n",
        "folder_path = '/content/drive/MyDrive/data_argi_llm/kvk_pop/LAKSHADWEEP/lakshadweep/lakshadweep'"
      ],
      "metadata": {
        "id": "BBNooIMf72te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka-D7W7s-0Gz"
      },
      "outputs": [],
      "source": [
        "# Function to read and extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    text = \"\"\n",
        "    with open(pdf_file, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Loop through each file in the folder and extract text if it's a PDF\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.pdf'):\n",
        "        pdf_path = os.path.join(folder_path, filename)\n",
        "        pdf_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "        # Save the extracted text to a text file with the same name as the PDF\n",
        "        output_filename = filename.replace('.pdf', '.txt')\n",
        "        output_path = os.path.join(folder_path, output_filename)\n",
        "        with open(output_path, 'w') as output_file:\n",
        "            output_file.write(pdf_text)\n",
        "        print(f\"Extracted text from {filename} and saved to {output_filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creation of dataset using extracted text - if required"
      ],
      "metadata": {
        "id": "M5s9T6IX_0Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "bHO-FaNb_pBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "GIWRtrKxHs2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the model and tokenizer with 4-bit quantization\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load the model with 4-bit precision using BitsAndBytesConfig\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',  # Automatically map to available devices (GPU/CPU)\n",
        "    token=token\n",
        ")\n"
      ],
      "metadata": {
        "id": "eiwXiv9cG5o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the input text from text file\n",
        "file_list = [f for f in os.listdir(folder_path)\\\n",
        "             if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.txt')]\n",
        "with open(os.path.join(folder_path, file_list[0]), 'r') as file:\n",
        "    input_text = file.read()\n",
        "\n",
        "# Split input text into smaller chunks (e.g., 2000 characters each)\n",
        "chunk_size = 2000\n",
        "text_chunks = [input_text[i:i + chunk_size] for i in range(0, len(input_text),\\\n",
        "                                                           chunk_size)]\n"
      ],
      "metadata": {
        "id": "FJS6bUS6I-Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a list to store formatted responses\n",
        "formatted_responses = []\n",
        "\n",
        "# Process each chunk and generate instruction-response pairs\n",
        "for chunk in text_chunks:\n",
        "    prompt = f\"Extract all possible questions and answers from the following text:\\n\\n{chunk}\\n\\nFormat them as 'Q: <question>' and 'A: <answer>'.\"\n",
        "\n",
        "    # Tokenize the prompt into pytorch tensors\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Calculate the length of the prompt in tokens\n",
        "    prompt_length = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "    # Move the input tensors to the same device (machine) as the model\n",
        "    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
        "\n",
        "    # Generate outputs with increased max_new_tokens\n",
        "    outputs = model.generate(**inputs, max_new_tokens=prompt_length+600, num_return_sequences=1)\n",
        "\n",
        "    # Slice the generated output to remove the initial prompt tokens\n",
        "    output = outputs[0][prompt_length:]\n",
        "\n",
        "    # Decode and process the generated response\n",
        "    response = tokenizer.decode(output, skip_special_tokens=True)\n",
        "\n",
        "    # Extract only lines starting with \"Q:\" or \"A:\"\n",
        "    questions_answers = []\n",
        "    for line in response.split('\\n'):\n",
        "        if line.startswith(\"Q:\") or line.startswith(\"A:\"):\n",
        "            questions_answers.append(line.strip())\n",
        "    formatted_responses.extend(questions_answers)"
      ],
      "metadata": {
        "id": "LCLHOjYcIh_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the formatted responses\n",
        "for item in formatted_responses:\n",
        "    print(item)"
      ],
      "metadata": {
        "id": "dKwkQg_i_p4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the formatted responses to a file for training\n",
        "with open(os.path.join(folder_path, 'data_ft.txt'), 'w') as f:\n",
        "    for item in formatted_responses:\n",
        "        f.write(f\"{item}\\n\")"
      ],
      "metadata": {
        "id": "yKREGxrkAK-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}