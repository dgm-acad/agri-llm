{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3dLbZ6tcWVUn",
        "zJVbVdms_X2O",
        "M5s9T6IX_0Qr",
        "BwTVmDsPARFT",
        "KUgGcUF3A8ga"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fPP77DqlWQLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Start"
      ],
      "metadata": {
        "id": "3dLbZ6tcWVUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XwIh0yWWimv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d12b121-a6f1-4240-830a-bf5715952ef1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load token from file\n",
        "import os\n",
        "with open('/content/drive/MyDrive/hf_token.txt', 'r') as file:\n",
        "    hf_token = file.read().strip()"
      ],
      "metadata": {
        "id": "PAUpUj0swDyC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetuning"
      ],
      "metadata": {
        "id": "BwTVmDsPARFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets bitsandbytes peft"
      ],
      "metadata": {
        "id": "UlJUwLusAMDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, Dataset\n",
        "import torch\n",
        "import bitsandbytes as bnb\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "folder_path = \"/content/drive/MyDrive/data_argi_llm/kvk_pop/LAKSHADWEEP/lakshadweep/lakshadweep\""
      ],
      "metadata": {
        "id": "T3GkvtlwCjrL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and dataset configuration\n",
        "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "dataset_path = os.path.join(folder_path, 'data_ft.txt')\n",
        "new_model = \"Llama-2-7b-chat-ft\""
      ],
      "metadata": {
        "id": "E5pfGdUTzSnW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model with 4-bit precision and configure LoRA\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    use_auth_token=hf_token\n",
        ")\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configure LoRA parameters and apply LoRA to the model\n",
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.config.use_cache = False\n"
      ],
      "metadata": {
        "id": "LlYpjjEsDRV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = load_dataset(\"text\", data_files=dataset_path, split=\"train\")\n",
        "\n",
        "# Function to format the dataset for training\n",
        "def format_dataset(example):\n",
        "    # Extract the relevant parts from the string\n",
        "    text = example['text']\n",
        "    return {\n",
        "        'input_ids': tokenizer(text, padding='max_length', truncation=True, max_length=512).input_ids,\n",
        "        'labels': tokenizer(text, padding='max_length', truncation=True, max_length=512).input_ids\n",
        "    }\n",
        "\n",
        "# Format the dataset\n",
        "dataset = dataset.map(format_dataset, remove_columns=[\"text\"])"
      ],
      "metadata": {
        "id": "eIh_RCPJLZEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(folder_path, 'results'),\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    logging_steps=25,\n",
        "    max_grad_norm=0.3,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    save_steps=0,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start fine-tuning\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "9BPllaG9AL67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model\n",
        "trainer.model.save_pretrained(os.path.join(folder_path, new_model))\n",
        "tokenizer.save_pretrained(os.path.join(folder_path, new_model))"
      ],
      "metadata": {
        "id": "GNy3GnKuBFyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output"
      ],
      "metadata": {
        "id": "KUgGcUF3A8ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pipeline"
      ],
      "metadata": {
        "id": "QUA1AYl0AquU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "rNPp_DkgzCH4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model and tokenizer with quantization\n",
        "model_name = os.path.join(folder_path, new_model)  # Ensure this path is correct and contains required files\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load model with 4-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,  # Directly specify loading in 4-bit mode\n",
        "    torch_dtype=torch.float16  # Set the compute dtype\n",
        ")\n",
        "\n",
        "# Run text generation pipeline with our fine-tuned model\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "\n",
        "# Function to generate response\n",
        "def generate_response(prompt):\n",
        "    result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "    return result[0]['generated_text']\n",
        "\n",
        "# Example usage\n",
        "prompt = \"What is the bio bin made of?\"\n",
        "output = generate_response(prompt)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "QymX6MzzAtI9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}